{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes\n",
        "\n",
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        " - Information Gain (IG) is a metric that measures how much “uncertainty” in the dataset reduces after splitting on a feature. Decision Trees use Information Gain to choose the best attribute for splitting. It is calculated as the difference between the entropy before a split and the weighted entropy after the split. A feature with the highest Information Gain is selected at each node because it gives the purest child nodes. IG helps the tree grow in a way that best separates the classes. It is mainly used in the ID3 algorithm.\n",
        "\n",
        "2. What is the difference between Gini Impurity and Entropy?\n",
        " - Gini Impurity measures how often a randomly chosen sample would be incorrectly labeled. Entropy measures the level of uncertainty or disorder in the data. Gini is simpler and faster computationally, making it preferred in CART decision trees.\n",
        "\n",
        "    Entropy is more mathematically complex and tends to create slightly deeper trees. Both measure impurity, but Gini usually creates better-balanced splits while Entropy focuses more on pure partitions.\n",
        "\n",
        "3. What is Pre-Pruning in Decision Trees ?\n",
        " - Pre-pruning stops the tree from growing too deep by applying constraints during training. Common techniques include limiting the maximum depth, minimum samples per split, or minimum leaf size. It prevents overfitting by halting splitting when the improvement becomes insignificant. Pre-pruning ensures the tree remains generalizable to unseen data. It also reduces training time and model complexity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AUTvCWNWds96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = pd.Series(model.feature_importances_, index=data.feature_names)\n",
        "print(\"Feature Importances:\\n\", feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxcEq8krfHhM",
        "outputId": "2632af69-1893-4474-d752-37eb350e12fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            " sepal length (cm)    0.026667\n",
            "sepal width (cm)     0.000000\n",
            "petal length (cm)    0.050723\n",
            "petal width (cm)     0.922611\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        " - SVM is a supervised machine learning algorithm used for classification and regression. It finds the best boundary (hyperplane) that maximizes the margin between classes. SVM works well in high-dimensional spaces and is robust to outliers. It can handle linear as well as non-linear decision boundaries using kernel functions. SVM is popular for text classification, image recognition, and bioinformatics.\n",
        "\n",
        "6. What is the Kernel Trick in SVM?\n",
        " - The Kernel Trick allows SVM to solve non-linear problems by projecting data into a higher-dimensional space without explicitly doing the transformation. This mathematical shortcut makes computation fast and efficient. Common kernels are Linear, Polynomial, and RBF. It helps SVM draw complex boundaries between classes. Without the kernel trick, SVM could only solve linear classification problems.\n",
        ""
      ],
      "metadata": {
        "id": "65VxT7v3fVGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcSyjmfjfxh-",
        "outputId": "20556cd6-c41f-4f8c-a8ad-ad178c71f519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the Naive Bayes classifier, and why is it called \"Naive\"?\n",
        " - Naive Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that all features are statistically independent from each other. This assumption is unrealistic in real datasets, which is why the model is called “naive.” Despite this simplification, Naive Bayes performs extremely well, especially for text classification. It is fast, scalable, and works well with high-dimensional data.\n",
        "\n",
        "9.  Explain the differences between Gaussian Naive Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naive Bayes\n",
        " - Gaussian NB: Used when features are continuous and follow normal distribution (e.g., age, height).\n",
        "    \n",
        "    Multinomial NB: Used for count data such as word frequencies in NLP (bag-of-words model).\n",
        "\n",
        "    Bernoulli NB: Used when features are binary (0/1), such as the presence or absence of a word.\n",
        "    \n",
        "    Each variant uses Bayes’ theorem but applies a different likelihood function depending on the data type.\n"
      ],
      "metadata": {
        "id": "O2PjFEOOgCEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian NB model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions & accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1vDoUOSgmUt",
        "outputId": "d7df2cec-b42f-4202-fb22-99af3ddde6ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}